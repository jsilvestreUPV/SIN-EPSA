{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T3.4 Ejercicios sobre Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Sea un modelo de regresión logística en notación compacta (homogénea) para un problema de clasificación en $C=3$ clases y datos representados mediante vectores de dimensión $D=2$\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(\\boldsymbol{y}\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{W}=\\begin{pmatrix}0&0&0\\\\1&-1&0\\\\1&1&0\\end{pmatrix}\\in\\mathbb{R}^{(D+1)\\times C}$$\n",
    "La probabilidad $P$ de que $\\boldsymbol{x}=(1, 0.5, 0.5)^t$ pertenezca a la clase $1$ és:\n",
    "1. $P < 0.25$\n",
    "2. $0.25\\leq P < 0.5$\n",
    "3. $0.5\\leq P < 0.75$\n",
    "4. $0.75\\leq P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:** $\\;$ la 3\n",
    "$$p(y=1\\mid\\boldsymbol{x};\\mathbf{W})=\\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x})_1=\\mathcal{S}\\left(\\begin{pmatrix}0&1&1\\\\0&-1&1\\\\0&0&0\\end{pmatrix}\\begin{pmatrix}1\\\\0.5\\\\0.5\\end{pmatrix}\\right)_1=\\mathcal{S}((1, 0, 0)^t)_1=\\dfrac{e}{e+2}=0.5761$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Sea un modelo de regresión logística en notación compacta (homogénea) para un problema de clasificación en $C=3$ clases y datos representados mediante vectores de dimensión $D=2$\n",
    "$$p(\\boldsymbol{y}\\mid\\boldsymbol{x};\\mathbf{W})=\\operatorname{Cat}(\\boldsymbol{y}\\mid \\mathcal{S}(\\mathbf{W}^t\\boldsymbol{x}))\n",
    "\\quad\\text{con}\\quad%\n",
    "\\mathbf{W_0}=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}\\in\\mathbb{R}^{(D+1)\\times C}$$\n",
    "Actualiza $\\mathbf{W}_0$ mediante una iteración de descenso por gradiente con conjunto de entrenamiento $\\,\\mathcal{D}=\\{(\\boldsymbol{x_1}=(1, 1, 1)^t, y_1=1)\\}\\,$ y factor de aprendizaje $\\,\\eta=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{a}_1%\n",
    "&=\\mathbf{W}_0^t\\boldsymbol{x}_1%\n",
    "=\\begin{pmatrix}1&-1&0\\\\0&1&0\\\\1&-1&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}%\n",
    "=\\begin{pmatrix}0\\\\1\\\\1\\end{pmatrix}\\\\[3mm]%\n",
    "\\boldsymbol{\\mu}_1%\n",
    "&=S(\\boldsymbol{a}_1)%\n",
    "=\\left( \\frac{e^0}{1+2e}, \\frac{e^1}{1+2e}, \\frac{e^1}{1+2e}\\right)^t\n",
    "=\\begin{pmatrix}0.1554\\\\0.4222\\\\0.4222\\end{pmatrix}\\\\[3mm]%\n",
    "\\mathbf{W}_1%\n",
    "&=\\mathbf{W}_0 - \\eta \\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_0} \\\\%\n",
    "&=\\mathbf{W}_0 - \\eta \\left( \\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t \\right)\\\\%\n",
    "&=\\mathbf{W}_0-\\eta\\,\\boldsymbol{x}_1(\\boldsymbol{\\mu}_1-\\boldsymbol{y}_1)^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}\\left( \\begin{pmatrix}0.1554\\\\0.4222\\\\0.4222\\end{pmatrix} - \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix} \\right)^t\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-0.1\\,\\begin{pmatrix}1\\\\1\\\\1\\end{pmatrix}(-0.8446, 0.4222, 0.4222)\\\\%\n",
    "&=\\begin{pmatrix}1&0&1\\\\-1&1&-1\\\\0&0&1\\end{pmatrix}-\\begin{pmatrix}-0.0845&0.0422&0.0422\\\\-0.0845&0.0422&0.0422\\\\-0.0845&0.0422&0.0422\\end{pmatrix}\\\\%\n",
    "&=\\begin{pmatrix}1.0845&-0.0422&0.9578\\\\-0.9155&0.9578&-1.0422\\\\0.0845&-0.0422&0.9578\\end{pmatrix}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "La siguiente tabla presenta un conjunto de $2$ muestras de entrenamiento de $2$ dimensiones procedentes de $2$ clases:\n",
    "\n",
    "<center>\n",
    "\n",
    "|$n$|$x_{n1}$|$x_{n2}$|$c_n$|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|1|1|0|1|\n",
    "|2|1|1|2|\n",
    "\n",
    "</center>\n",
    "\n",
    "Adicionalmente, la siguiente tabla representa una matriz de pesos iniciales con los pesos de cada clase dispuestos por columnas:\n",
    "\n",
    "<center>\n",
    "\n",
    "|$w_1$|$w_2$|\n",
    "|:-:|:-:|\n",
    "|0|0|\n",
    "|0|0|\n",
    "|-0.25|0.25|\n",
    "\n",
    "</center>\n",
    "\n",
    "Se pide:\n",
    "\n",
    "1. Calcula el vector de logits asociado a cada muestra de entrenamiento.\n",
    "1. Aplica la función softmax al vector de logits de cada muestra de entrenamiento.\n",
    "1. Clasifica todas las muestras de entrenamiento. En caso de empate, elige cualquier clase.\n",
    "1. Calcula el gradiente de la función NLL en el punto de la matriz de pesos iniciales.\n",
    "1. Actualiza la matriz de pesos iniciales aplicando descenso por gradiente con factor de aprendizaje $\\eta=1.0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución\n",
    "\n",
    "0. Recabamos datos y los preparamos debidamente para aplicar una iteración del algoritmo de descenso por gradiente. Concretamente: pasamos las $N$ muestras de entrenamiento a notación homogénea $\\; \\boldsymbol{x}_n \\in \\mathbb{R}^{D+1}$, pasamos las $N$ etiquetas de clase de las muestras a codificación *one-hot* $\\;y_n \\in [0,1]^{C}$, y construimos la matriz de pesos iniciales $\\mathbf{W}_0 \\in \\mathbb{R}^{(D+1)\\times C}$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&N=2; \\quad D=2; \\quad C=2 \\\\[0.25cm]\n",
    "&\\boldsymbol{x_1} = (1,1,0)^t; \\quad \\boldsymbol{y_1} = (1,0)^t; \\\\[0.25cm]\n",
    "&\\boldsymbol{x_2} = (1,1,1)^t; \\quad \\boldsymbol{y_2} = (0,1)^t; \\\\[0.25cm]\n",
    "\n",
    "&\\mathbf{W}_0 = \\begin{pmatrix} 0 & 0  \\\\ 0 & 0 \\\\ -0.25 & 0.25 \\end{pmatrix}\\\\\n",
    "\n",
    "&\\eta = 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "1. Cálculo de los vectores de logits $\\boldsymbol{a_1}$ y $\\boldsymbol{a_2}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{a_1} &= \\mathbf{W}_0^t \\boldsymbol{x_1} = \\begin{pmatrix} 0 & 0  & -0.25\\\\ 0 & 0  & 0.25 \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\ 1\\\\ 0\\end{pmatrix} = (0,0)^t\\\\\n",
    "\n",
    "\\boldsymbol{a_2} &= \\mathbf{W}_0^t \\boldsymbol{x_2} = \\begin{pmatrix} 0 & 0  & -0.25\\\\ 0 & 0  & 0.25 \\end{pmatrix} \\cdot \\begin{pmatrix} 1\\\\ 1\\\\ 1\\end{pmatrix} = (-0.25,0.25)^t\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. Calculo de los vectores de probabilidades $\\boldsymbol{\\mu}_1$ y $\\boldsymbol{\\mu}_2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu}_1 &= \\mathcal{S}(\\boldsymbol{a_1}) = \\left(\\frac{e^{0}}{e^0+e^0}, \\frac{e^{0}}{e^0+e^0}\\right)^t = (0.5, 0.5)^t \\\\\n",
    "\\boldsymbol{\\mu}_2 &= \\mathcal{S}(\\boldsymbol{a_2}) = \\left(\\frac{e^{-0.25}}{e^{-0.25}+e^{0.25}}, \\frac{e^{0.25}}{e^{-0.25}+e^{0.25}}\\right)^t = (0.38, 0.62)^t \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "3. Clasificación de las muestras de entrenamiento $\\boldsymbol{x_1}$ y $\\boldsymbol{x_2}$:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\hat{c}(\\boldsymbol{x_1}) &= \\operatorname*{argmax}\\limits_{c} \\; \\mu_{1c} = \\operatorname*{argmax}\\limits_{c} \\; [0.5, 0.5] = 1 \\\\\n",
    "\n",
    "\\hat{c}(\\boldsymbol{x_2}) &= \\operatorname*{argmax}\\limits_{c} \\; \\mu_{2c} = \\operatorname*{argmax}\\limits_{c} \\; [0.38, 0.62] = 2 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "4. Cálculo del gradiente de la NLL evaluado en $\\mathbf{W}_0$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}^t}\\biggr\\vert_{\\mathbf{W}_0} &= \\frac{1}{N}\\sum_{n=1}^N\\boldsymbol{x}_n(\\boldsymbol{\\mu}_n-\\boldsymbol{y}_n)^t \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left(\\boldsymbol{x}_1(\\boldsymbol{\\mu}_1-\\boldsymbol{y}_1)^t + \\boldsymbol{x}_2(\\boldsymbol{\\mu}_2-\\boldsymbol{y}_2)^t \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left( \\begin{pmatrix} 1\\\\ 1\\\\ 0\\end{pmatrix} \\cdot (-0.5, 0.5) + \\begin{pmatrix} 1\\\\ 1\\\\ 1\\end{pmatrix} \\cdot (0.38, -0.38)  \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\left( \\begin{pmatrix} -0.5 & 0.5\\\\ -0.5 & 0.5\\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0.38 & -0.38\\\\ 0.38 & -0.38\\\\ 0.38 & -0.38 \\end{pmatrix} \\right) \\\\\n",
    "                &= \\frac{1}{2} \\cdot \\begin{pmatrix} -0.12 & 0.12\\\\ -0.12 & 0.12\\\\ 0.38 & -0.38 \\end{pmatrix} \\\\\n",
    "                &= \\begin{pmatrix} -0.06 & 0.06\\\\ -0.06 & 0.06\\\\ 0.19 & -0.19 \\end{pmatrix} \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "5. Obtención de la matriz de pesos actualizada $\\mathbf{W}_1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\mathbf{W}_1 &= \\mathbf{W}_0 - \\eta \\frac{\\partial\\operatorname{NLL}}{\\partial\\mathbf{W}}\\biggr\\vert_{\\mathbf{W}_0} \\\\ \n",
    "               &= \\begin{pmatrix} 0 & 0  \\\\ 0 & 0 \\\\ -0.25 & 0.25 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} -0.06 & 0.06\\\\ -0.06 & 0.06\\\\ 0.19 & -0.19 \\end{pmatrix} \\\\\n",
    "               &= \\begin{pmatrix} 0.06 & -0.06  \\\\ 0.06 & -0.06 \\\\ -0.44 & 0.44 \\end{pmatrix}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sin-2324",
   "language": "python",
   "name": "sin-2324"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
